```python
# 螺丝与螺母配对问题：
# 有一组 n 个螺丝和 n 个螺母，每个螺丝都只与唯一一个螺母配对，两组元素的尺寸各不相同但一一对应。
# 你只能使用一个螺丝和一个螺母进行比较（即判断螺丝是否大于、小于或等于螺母），不能直接比较两个螺丝或两个螺母的大小。
# 请设计一个算法，将两个数组都排序，并使得第 i 个螺丝和第 i 个螺母配对。
# 输入：
# nuts：长度为 n 的整数数组，螺丝尺寸
# bolts：长度为 n 的整数数组，螺母尺寸，螺丝和螺母均包含相同的一组数字，乱序
# 输出：
# 按照尺寸排序后的螺丝和螺母数组，每对一一配对

# 利用快速排序分治思想，每次用一个螺丝作为枢轴，将所有螺母partition（只比较螺丝和螺母）；然后用找到的匹配螺母作为枢轴对螺丝分组。递归左右区间即可。
# 做法类似于快速排序的partition步骤，但每次分组比较都必须是“螺丝vs螺母”的异种比较。
# 递归处理。

nuts = [4, 2, 1, 3]
bolts = [3, 2, 4, 1]
# 排序后=> [1, 2, 3, 4]
# 对应配对：螺丝1-螺母1、螺丝2-螺母2、...

def partition(arr, low, high, pivot, compare):
    i = low
    for j in range(low, high+1):
        if compare(arr[j], pivot) < 0:
            arr[i], arr[j] = arr[j], arr[i]
            i += 1
        elif compare(arr[j], pivot) == 0:
            arr[j], arr[high] = arr[high], arr[j]
            j -= 1
    arr[i], arr[high] = arr[high], arr[i]
    return i

def compare_bolt(bolt, nut):
    return (bolt > nut) - (bolt < nut)
def compare_nut(nut, bolt):
    return (nut > bolt) - (nut < bolt)

def match_nuts_and_bolts(nuts, bolts, low, high):
    if low < high:
        pivot_nut = nuts[low]
        pivot_index = partition(bolts, low, high, pivot_nut, compare_bolt)

        pivot_bolt = bolts[pivot_index]
        partition(nuts, low, high, pivot_bolt, compare_nut)

        match_nuts_and_bolts(nuts, bolts, low, pivot_index-1)
        match_nuts_and_bolts(nuts, bolts, pivot_index+1, high)

nuts = [4, 2, 1, 3]
bolts = [3, 2, 4, 1]

match_nuts_and_bolts(nuts, bolts, 0, len(nuts)-1)
print("螺丝：", nuts)
print("螺母：", bolts)
for i in range(len(nuts)):
    print(f"螺丝 {nuts[i]} <=> 螺母 {bolts[i]}")
        

```

```python
def get_result(n, epsilon=1e-2):
    x = n / 3
    while True:
        grad = 3 * (x ** 2) 
        x_new = x - (x ** 3 - n) / grad
        if abs(x - x_new) < epsilon:
            break
        x = x_new
    return x
print(get_result(35))
print(3.2 ** 3)

# def get_result(n, lr=0.00001, epochs=10000, epsilon=1e-2):
#     x = n / 3
#     for i in range(epochs):
#         grad = 6 * (x ** 2) * ((x ** 3) - n)
#         x = x - lr * grad
#     return round(x, 2)
# print(get_result(16))
```
```python
def count_inversions(nums):
    def merge_sort(arr):
        if len(arr) <= 1:
            return arr, 0
        mid = len(arr) // 2
        left, inv_left = merge_sort(arr[:mid])
        right, inv_right = merge_sort(arr[mid:])
        merged = []
        i = j = inv_count = 0
        # 合并两个有序数组的同时计数
        while i < len(left) and j < len(right):
            if left[i] <= right[j]:
                merged.append(left[i])
                i += 1
            else:
                merged.append(right[j])
                # left[i] > right[j] 时，left[i:] 都与 right[j] 构成逆序对
                inv_count += len(left) - i
                j += 1
        merged += left[i:]
        merged += right[j:]
        return merged, inv_left + inv_right + inv_count
    _, total_inversions = merge_sort(nums)
    return total_inversions

# 示例
nums = [2, 4, 1, 3, 5]
print(count_inversions(nums))  # 输出：3   逆序对 (2,1), (4,1), (4,3)

```
```python
# 1. 把输入和上个隐藏层拼一起，过线性，生成2组门的值
# 2. candidates = tanh * 候选层（[h_prev * 重置门, x]）
# 3. 输出 = (1 - 更新门) * candidates + 更新门 * h_prev
import torch
import torch.nn as nn

class GRU(nn.Module):
    def __init__(self, input_size, hidden_size):
        super().__init__()
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.linear_gates = nn.Linear(input_size + hidden_size, 2 * hidden_size)
        self.linear_candidate = nn.Linear(input_size + hidden_size, hidden_size)

    def forward(self, x, h_prev):
        combined = torch.cat([x, h_prev], dim=1)
        gates = self.linear(combined)
        r, z = gates.chunk(2, dim=1)

        r = torch.sigmoid(r)
        z = torch.simoid(z)

        candidate_input = torch.cat([x, r * h_prev], dim=1)
        n = torch.tanh(self.linear_candidate(candidate_input)) 

        h = (1 - z) * n + z * h_prev
        return h
```
```python
# 1. 把输入和上个隐藏层拼一起，过线性，生成4组门的值
# 2. cell状态 = 上次cell * 遗忘门 + 输入门 * 新内容
# 3. 输出门筛选： 当前隐藏状态 = 输出门 * tanh(cell状态)
import torch
import torch.nn as nn

class LSTM(nn.Module):
    def __init__(self, input_size, hidden_size):
        super().__init__()
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.linear = nn.Linear(input_size + hidden_size, 4 * hidden_size)
    
    def forward(self, x, h_prev, c_prev):
        combined = torch.cat([x, h_prev], dim=1)
        gates = self.linear(combined)
        i, f, g, o = gates.chunk(4, dim=1)

        i = torch.sigmoid(i)
        f = torch.sigmoid(f)
        g = torch.tanh(g)
        o = torch.sigmoid(o)

        c = f * c_prev + i * g
        h = o * torch.tanh(c)
        return h, c
```

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class GroupQueryAttention(nn.Module):
    def __init__(self, num_head, num_group, hidden_dim):
        super().__init__()
        self.hidden_dim = hidden_dim
        self.num_head = num_head
        self.num_group = num_group
        assert self.num_head % self.num_group == 0
        self.d_k = hidden_dim // num_head

        self.w_q = nn.Linear(hidden_dim, hidden_dim)
        self.w_o = nn.Linear(hidden_dim, hidden_dim)

        self.w_k = nn.Linear(hidden_dim, self.d_k * num_group)
        self.w_v = nn.Linear(hidden_dim, self.d_k * num_group)

    def forward(self, x, mask=None):
        batch_size, seq_len, hidden_dim = x.size()
        q = self.w_q(x).view(batch_size, seq_len, self.num_head, self.d_k).transpose(1, 2)
        k = self.w_k(x).view(batch_size, seq_len, self.num_group, self.d_k)
        v = self.w_v(x).view(batch_size, seq_len, self.num_group, self.d_k)

        head_per_group = self.num_head // self.num_group

        k = k.repeat_interleave(head_per_group, dim=2).transpose(1, 2)
        v = v.repeat_interleave(head_per_group, dim=2).transpose(1, 2)

        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / (self.d_k ** 0.5)
        if mask is not None:
            attn_scores = attn_scores.masked_fill(mask==0, float('-inf'))
        attn = torch.softmax(attn_scores, dim=-1)
        out = torch.matmul(attn, v)
        out = out.transpose(1, 2).contiguous().view(batch_size, seq_len, hidden_dim)
        out = self.w_o(out)
        return out
```
```python
import torch
import torch.nn as nn

class BatchNorm1d(nn.Module):
    def __init__(self, num_features, eps=1e-5, momentum=0.1):
        super().__init__()
        self.gamma = nn.Parameter(torch.ones(num_features))
        self.beta = nn.Parameter(torch.zeros(num_features))
        self.eps = eps
        self.momentum = momentum
        #register_buffer 用于向 nn.Module 注册一个 不会被优化器更新但会随模型保存/加载、GPU/CPU切换同步的（非参数型）状态变量
        self.register_buffer('running_mean', torch.zeros(num_features))
        self.register_buffer('running_var', torch.ones(num_features))
    
    def forward(self, x):
        if self.training:
            batch_mean = x.mean(dim=0, keepdim=True)
            batch_var = x.var(dim=0, keepdim=True, unbiased=False)

            self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * batch_mean
            self.running_var = (1 - self.momentum) * self.running_var + self.momentum * batch_var

            mean = batch_mean
            var = batch_var
        else:
            mean = self.running_mean
            var = self.running_var
        x_norm = (x - mean)/torch.sqrt(var + self.eps)
        y = self.gamma * x_norm + self.beta
        return y

```
```python
# LayerNorm（层归一化）通常不需要区分训练（train）和推理（eval）阶段，与BatchNorm不同。
# 直接调用
import torch.nn as nn
layernorm = nn.LayerNorm(hidden_dim, eps=1e-5)
output = layernorm(x)

# 代码实现
class CustomLayerNorm(nn.Module):
    def __init__(self, hidden_dim, eps=1e-5):
        super().__init__()
        self.gamma = nn.Parameter(torch.ones(hidden_dim))
        self.beta = nn.Parameter(torch.zeros(hidden_dim))
        self.eps = eps

    def forward(self, x):
        # x shape: [batch_size, seq_len, hidden_dim]
        mean = x.mean(dim=-1, keepdim=True)
        var = x.var(dim=-1, keepdim=True, unbiased=False)
        x_norm = (x - mean) / torch.sqrt(var + self.eps)
        # 广播加权
        out = self.gamma * x_norm + self.beta
        return out
```
```python
import torch
import torch.nn as nn
import torch.nn.Functional as F

class MultiHeadAttention(nn.Module):
    def __init__(self, n_head, hidden_dim):
        super().__init__()
        self.n_head = n_head
        self.d_k = hidden_dim // n_head
        self.w_q = nn.Linear(hidden_dim, hidden_dim)
        self.w_k = nn.Linear(hidden_dim, hidden_dim)
        self.w_v = nn.Linear(hidden_dim, hidden_dim)
        self.w_o = nn.Linear(hidden_dim, hidden_dim)
    
    def forwart(self, x, mask=None):
        batch_size, seq_len, hidden_dim = x.size()
        q = self.w_q(x).view(batch_size, seq_len, self.n_head, -1).transpose(1, 2)
        k = self.w_k(x).view(batch_size, seq_len, self.n_head, -1).transpose(1, 2)
        v = self.w_v(x).view(batch_size, seq_len, self.n_head, -1).transpose(1, 2)
        
        scores = torch.matmul(q, k.transpose(-1, -2))/self.d_k ** 0.5
        if mask is not None:
            scores = scores.masked_fill(mask==0, float('-inf'))
        attention = F.softmax(scores, dim=-1)

        output = torch.matmul(attention, v)
        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, -1)
        output = self.w_o(output)
        return output
```
```python

# AUC 其实在二分类场景下可理解为：
# 随机取一个正样本和一个负样本，预测分数高于负样本的概率。
# 最暴力实现方式是枚举所有正-负对，但这会是O(M*N)。
# 排序+rank法通过一次排序，将统计过程优化为O(nlogn)。

# 步骤：
# 1. 全部样本按预测分数升序排序（分数越高排后，分数低排前）。
# 2. 分配rank：排序后第k个样本的rank为k+1（即索引+1）。
# 3. 统计所有正样本的rank之和（记作 S）。
# 由于正样本分布越靠后（分数越高、rank越高），表明模型判别越准。
#（https://zhuanlan.zhihu.com/p/8263124740）
import numpy as np
def auc(labels, scores):
    n = len(labels)
    labels = np.array(labels)
    scores = np.array(scores)

    pos = np.sum(labels==1)
    neg = np.sum(labels==0)

    if pos == 0 or neg == 0:
        return None
    
    sorted_idx = np.argsort(scores)
    sorted_labels = labels[sorted_idx]
    
    ranks = np.arange(n) + 1
    
    pos_ranks_sum = np.sum(ranks[sorted_labels == 1])
    auc = (pos_ranks_sum - pos*(pos + 1)/2)/ (pos * neg)
    return auc
```

1. 你遇到过GNN过平滑问题吗？如何优化？
    a. 引入残差（Residual，Skip Connection）
    b. 特征归一化/BatchNorm/LayerNorm
    c. 限制层数/局部感受野优化
2. GNN和传统CNN/MLP的本质区别？
    a. MLP：层间全连接，不考虑不同样本之间的交互，只针对自身信息进行非线性映射。
    b. CNN：向局部空间区域聚合特征，通过卷积核滑动窗口，捕捉邻近像素间关联。只有固定窗口范围内的信息传递。
    c. GNN：节点嵌入更新依赖邻居节点特征的聚合。通过消息传递机制（message passing），每个节点与一阶（甚至多阶）邻居反复交换信息，捕获复杂的结构和关联模式，传播范围与图结构有关。
3. 图注意力机制和Transformer注意力机制的区别
    a. 局部注意力 vs 全局注意力
4. GNN 和 RW的区别
    a. GNN和RW都可以用于图数据的节点表示学习，但本质区别在于：GNN通过参数化神经网络结构，显式聚合邻居特征和结构信息，具备端到端训练和表达能力；而随机游走只依赖于图的拓扑结构，通过模拟节点上的随机路径来捕捉结构关系，常与Word2vec等无监督算法配合使用，不能直接感知节点特征或利用监督信息。GNN更灵活、表达力更强，但计算消耗也更高，两者也可在部分应用中互相借鉴和结合。

● GIN（Graph Isomorphism Network）
    ○ 传统的图神经网络（GCN、GraphSAGE等）由于聚合和更新机制的限制，在区分某些结构不同但WL-test能区分的图时能力不足。GIN通过一种简单但有力的聚合方式，把判别力提升到与WL同构测试一样强
    ○ GIN用求和+MLP
    ○ 结构区分性强，但最适合图分类（所有节点整体表征），节点任务需具体看结构类型。

● GCN（Graph Convolutional Network）
    ○ 拉普拉斯矩阵归一化，特征卷积
    ○ CNN处理规则网格数据（如2D图片），GCN处理非规则图数据。卷积操作在GCN中是邻居节点特征聚合，而CNN是窗口滑动。
    ○ GCN归一化为何重要？解决节点度数差异带来的梯度消失/爆炸问题，提高特征传播的稳定性。
● GraphSAGE
    ○ 聚合函数可选（均值、池化、LSTM聚合）
        ■ 传统GCN每层需要聚合全部邻居节点的信息，但在大图（如社交网络，包含千万节点和边）这种“全邻居聚合”不现实
        ■ 对于每个目标节点v，只随机采样K个邻居节点（而非全部邻居）。通过聚合函数（如mean、LSTM、pooling等）汇总采样到的邻居节点特征。
        ■ 一般mean适合相似邻居聚合，LSTM和pooling适合异质性高以及存在重要节点等场景。可以根据任务实验选择
    ○ 支持归纳式学习（对未见节点/新图泛化）
● GAT（Graph Attention Network）
    ○ 使用注意力权重动态聚合邻居信息
    ○ 注意力分数是怎么学出来的？通过一个可训练的神经网络（通常单层全连接+LeakyReLU），端到端训练。

ReAct（Reasoning and Acting）是一种结合了推理（reasoning）和行动（acting）的方法，旨在提高大型语言模型（LLMs）在解决复杂任务时的性能和可解释性。ReAct的具体步骤如下：
● 定义任务和目标：首先，明确模型需要解决的任务和目标。这可能包括问题回答、事实验证、文本游戏或网页导航等。
● 生成推理轨迹：模型生成一系列推理轨迹，这些轨迹是模型内部的思考过程，用于解决问题。这些推理轨迹可能包括分解任务目标、提取重要信息、执行常识推理、指导搜索重构、跟踪进度和处理异常等。
● 执行行动：模型根据推理轨迹执行一系列行动。在ReAct中，行动是通过与外部环境（例如Wikipedia或其他知识库）的交互来完成的。行动可以是搜索信息、选择产品、选择选项或购买等。
● 交替推理和行动：在解决任务的过程中，模型会交替进行推理和行动。这意味着模型在执行每个行动后可能会生成新的推理轨迹，然后再执行新的行动，以此往复。
● 更新上下文：每次行动后，模型会更新上下文信息，这包括之前的行动、观察结果和新生成的推理轨迹。这种上下文更新帮助模型跟踪任务的当前状态，并为未来的推理和行动提供信息。
● 生成任务解决轨迹：通过上述步骤，模型生成一个包含行动、观察和推理轨迹的任务解决轨迹。这个轨迹不仅展示了模型如何解决问题，而且提供了模型决策的透明度，使得人类用户可以理解和评估模型的行为。
● 评估和调整：在实际应用中，模型生成的任务解决轨迹可能会被人类用户评估和调整。用户提供的反馈可以用来进一步指导模型的行为，或者在模型自身无法正确解决问题时进行干预。

拜读OneRec https://zhuanlan.zhihu.com/p/1918792219990689391
OneRec 由三个核心部分组成：分词器、编码器和解码器，外加一个在后续训练中用来微调的奖励系统。
    ○ 分词器负责把海量视频“切分”成一组固定的“语义 ID”，让模型不用记住每个视频对应的超大编号；
        ■ 视频的标题、标签、自动语音识别（ASR）、图片文字识别（OCR）、封面图以及几帧关键画面，送进miniCPM-V-8B 的大模型，得到高维的特征向量；
        ■ 再用一个轻量版的 QFormer，把这些高维表征压缩，既保留了信息，又方便后续处理
        ■ 构造了两类视频对：一类是“用户看过 A，也经常看 B”的行为对，另一类是内容上高度相似的对。在这些视频对上，他们用对比学习让压缩后的向量更贴近
        ■ 用 LLaMA3 对视频标题做语言建模，保证分词器不会把文字给“乱改掉”
        ■ 真正生成语义 ID：用 RQ-Kmeans（残差量化 K-means）的办法，把压缩向量分三次做聚类。第一次聚类得到第一层 ID，再用它算出残差，第二次聚类给出第二层 ID，如此递进，直到第三层。每个视频最终就变成了 {s1m, s2m, s3m} 这三级“粗—中—细”语义编号
    ○ 编码器则把用户的各种行为（看过哪些视频、停留多长时间、点赞评论等）都整理成一份浓缩的兴趣“画像”；
        ■ 第一是“用户静态特征”部分，就像档案卡，记录年龄、性别、地区等基本信息。
        ■ 第二是“短期行为”部分，抓取最近 20 条观看记录，不仅看视频编号，还关注作者、标签、时间、停留时长和点赞、评论等互动情况。
        ■ 第三是“高反馈”部分，会集中处理那些用户反应特别强烈的 256 条，比如反复观看、点赞、转发这些高参与度的内容。
        ■ 第四条“终身历史”路径，它要面对用户可能看过的上万、甚至十万条视频。OneRec 先用一种分层的 K-means 聚类，把相似的视频归到同一簇里，再从每个簇里选出最能代表那簇的item，把整个历史压缩成大约 2,000 条“精华”信息。接着再用轻量版的 QFormer（带 128 条可学查询向量）把这 2,000 条内容进一步浓缩，既抓住了长期兴趣的核心，又保证了计算可行。
    ○ 解码器就像在写故事一样，接着根据这份画像，一步步“写出”一串最符合用户口味的语义 ID，最后映射成具体的视频推荐。

        ■ 从一个可学习的起始符（BOS）开始，一步一步地“写出”视频的语义编号。具体来说，当模型要推荐某条视频时，它把“[开始]”加上这条视频的三级语义 ID 序列放在一起，然后按顺序预测下一个视频语义编号，就好像在写下一句话的下一个字。
        ■ 第一，用“因果注意力”让已经生成的编号彼此参考，好比先前写过的词会影响接下来要写的词；
        ■ 第二，用“交叉注意力”把用户兴趣的整体画像也拉进来，比方说把用户的历史偏好当作上下文，帮助模型判断下一步最合适的编号；
        ■ 第三，再把这条信息送进一个“混合专家”网络（MoE），这里面藏着很多小专家，模型会根据当前输入挑出排名前几的专家来帮忙，既能扩充模型容量，又不会让某个专家过载，保证所有专家都能被合理利用。
    ○ 强化学习：

        ■  先用一个小型神经网络，把点击、点赞、观看时长等多种反馈融合成一个“P-Score”，然后用一种叫 ECPO（Early Clipped GRPO）的算法，沿着这个分数不断优化模型，让推荐更“对胃口”。
        ■ 为了保证模型生成的视频 ID 串都是真实可用的，OneRec 在强化学习里还加了“格式奖励”。具体做法是：从一批生成结果中随机抽取若干条，把能对应到实际视频的标记当作奖励，非法或找不到视频的直接丢掉。这样，模型就学会只“写”那些能映射到真实内容的序列，不会再出现“空洞编号”。
        ■ 针对不同的业务场景，OneRec 还支持在奖励里加入“工业奖励”。举个例子，如果平台想适当压缩低质内容或提高新作者曝光，就可以在奖励函数里给这部分内容打上不同的加减分，让模型在统一的学习过程中自然兼顾这些商业或生态指标

KV Cache
KV Cache是Transformer标配的推理加速功能，transformer官方use_cache这个参数默认是True，但是它只能用于Decoder架构的模型，这是因为Decoder有Causal Mask，在推理的时候前面已经生成的字符不需要与后面的字符产生attention，从而使得前面已经计算的K和V可以缓存起来。
把之前的Key和Value缓存下来，只需计算新token的Key和Value，并把它拼到旧的缓存上，用于后续计算。

Prompt
● 任务型Prompt：如“请将下句翻译成英语：XXX”、“帮我写一个Python函数”
● 风格/语气Prompt：如“请用诗歌形式描述春天”，控制输出风格
● 少/零样本学习Prompt：包含示例或演示，激发模型在新任务上做迁移
● 多轮/上下文Prompt：拼接多轮历史对话或背景资料，引导模型理解交互上下文
Few-shot Prompt 是指在给大语言模型（LLM，比如GPT/Qwen等）输入提示（Prompt）的同时，在Prompt中嵌入少量人类示例（examples），帮助模型理解和迁移新任务，无需参数微调或大规模重训练。
Prompt Tuning 是一种针对大语言模型“微调”策略，但不用调整全部模型参数。它只在“Prompt Embedding”（一段可学习的软提示）上做参数学习，让模型产生特定行为，以效率和泛化能力兼顾为目标。
● 所谓“软Prompt”是指：一组可训练的embedding，作为模型输入前缀拼接上原始输入。
● 只训练Prompt，不动原模型。
● 流程：初始化一组“虚拟tokens”（Soft Prompt），嵌入到输入序列、在目标任务数据集（如分类/生成）上，仅优化Prompt参数，其余模型参数冻结、实现高效、少样本的任务适配
● 通常几万步就收敛，参数量非常小
In-context
In-context Learning ：模型通过在输入中直接给出任务描述、示例或历史对话等上下文信息，让模型理解并执行任务，而不需要更新或微调模型参数。
● Few-shot in-context learning：输入中嵌入几个“问题—答案”示例，最后给出新题，模型直接跟随示例风格输出。
● 多轮对话 in-context：输入拼接多轮人机对话记录，模型基于这些上下文自动完成下一轮回复。
● 多任务融合：用上下文信息指定任务类型，让模型在“原始参数冻结”情况下，灵活适应各种指令、表达或生成。
Instruction Tuning
在大模型已经完成通用预训练（如语言建模）后，用大规模人工设计的“任务指令—输入—期望输出”三元组数据，对模型做轻微微调，使其能够理解和执行多样化的自然语言指令。
● 数据收集：汇集/合成大量覆盖各类场景的“指令—输入—输出”（instruction, input, output）示例。
● 拼接训练样本：将三元组拼成对话或任务prompt，让模型在指令场景下进行监督学习。
CoT
让大模型在解决复杂问题时，模拟人类分步推理的技术，即在生成答案时不直接输出最终结果，而是展示清晰的逐步思考和中间过程，适用于数学、逻辑推理、常识问答、多步决策等任务。
● 自动cot生成：让大模型（如GPT-4/Qwen等）通过 prompt 生成多个带步骤答案（CoT），从原始问题自动扩展出推理链。
● Self-Consistency ：模型对同一个问题，通过多次独立推理链生成，最后用聚合/投票/集成方式选取最一致的答案，从而提升准确率与鲁棒性。
● Chain-of-Thought（CoT）不仅可以通过 prompt 在推理时激发，也常常用于有监督微调（Supervised Fine-Tuning, SFT）。GPT-3/4、Qwen、ChatGLM等在数学/逻辑/常识推理任务上普遍采用CoT Fine-tuning。

temperature/top-p/top-k
● 高温+大top-p：用于需要创新、多样、丰富内容的场景（如AI创作、对话助手）。
● 低温+小top-p/top-k：用于对输出稳定、准确要求较高的场景（如代码生成、知识问答、命令执行）。
温度是softmax分布的缩放参数，调控模型输出词分布的“平滑程度”。
● T=1：原始概率；
● T<1，分布更尖锐，模型更确定，输出更可预测；
● T>1，分布更平滑，随机性提升，多样性更高。

top-k : 每步只在最高概率的K个token中采样（其余概率置0再归一化）。
● 限制最可能token数量，防止罕见词被选，控制随机性
● K值越小，越保守，K大更丰富
top-p : 动态选取累计概率在p阈值内的最小token集合（而非固定K），然后归一化在这组内采样。
● 适配概率长尾，对高概率token集自适应截断，保证每次采样动态只考虑概率大于累积p的词
● 通常 p=0.8~0.95，自动聚焦头部token
● 低p更保守，高p更随机

post-norm : out = LayerNorm(x + SubLayer(x))
pre-norm : out = x + SubLayer(LayerNorm(x)) 
Pre-Norm的残差连接效应强，使得梯度的回传更容易，而Post-Norm的应用会使得梯度快速衰减，导致模型难以学习
● Pre-Norm可以显著提高深层网络（比如24层、48层大模型）的训练稳定性，减小梯度vanish/explode现象。Pre-Norm通常更容易收敛且支持更高的学习率，适合大规模深度训练。
● 在同参数量下，Post-Norm表现比Pre-Norm更好
● GPT-2/3及绝大多数大模型用Pre-Norm；BERT、原始Transformer用Post-Norm