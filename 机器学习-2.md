# 集成学习
## 1. 什么是集成学习算法？
集成学习（Ensemble Learning）是一种机器学习方法，通过组合（集成）若干个“弱模型”，减少偏差和方差、提升准确率、泛化能力强、避免过拟合，但计算资源消耗大，模型解释性差

- **Bagging（Bootstrap Aggregating）——“并行法”**
   - 典型算法：随机森林（Random Forest）
   - 原理：多次从训练集随机采样（有放回），训练多个模型，最后对结果投票或平均
   - 优势：降低方差，提高稳定性

- **Boosting——“串行法”**
   - 典型算法：Adaboost，梯度提升树（GBDT、XGBoost等）
   - 原理：模型串行训练，每个模型重点学习前面模型没学好的样本，模型间有加权
   - 优势：降低偏差，提高准确率

- **Stacking（堆叠、叠加学习）**
   - 原理：用多个不同类型模型做一级学习器，再把一级输出作为特征，交给另一个模型（二级学习器）做最终预测
   - 优势：融合多类模型优势

## 2. Bagging算法简介

Bagging流程
1. **数据重采样**  
   - 从原始训练集通过有放回抽样（Bootstrap），生成多份不同的子训练集。每份子集与原始数据规模相同但内容不同。

2. **训练多个模型**  
   - 用每份子集，分别训练一个独立的“基学习器”（通常是同一模型类型，如决策树）。

3. **模型整合**  
   - 对于分类任务，通常采用**投票法**（多数表决）；对于回归任务，采用**平均法**。
   - 最终输出为各模型结果整合后（如投票/平均）得到的预测。

Bagging的特点

- **并行训练**：基学习器可同步训练，计算效率高。
- **降低方差**：减少模型训练样本的偶然性，提升稳定性。
- **对弱学习器友好**：基学习器可以是不太复杂且容易出现过拟合的模型（如决策树）。

---

## 典型代表

- **随机森林**（Random Forest）：Bagging思想的最经典应用，基学习器为决策树，此外还对特征维度随机采样进一步增加多样性。

---

## 小结

> Bagging 通过多次采样、训练和集成，提升模型的稳健性，广泛应用于分类和回归问题，尤其对容易过拟合的模型非常有效。